<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>2.4.&nbsp;The Important Configurations</title><link rel="stylesheet" href="css/freebsd_docbook.css" type="text/css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.75.2"><link rel="home" href="book.html" title="The Apache HBase Book"><link rel="up" href="configuration.html" title="Chapter&nbsp;2.&nbsp;Configuration"><link rel="prev" href="log4j.html" title="2.3.&nbsp;log4j.properties"><link rel="next" href="client_dependencies.html" title="2.5.&nbsp;Client configuration and dependencies connecting to an HBase cluster"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">2.4.&nbsp;The Important Configurations</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="log4j.html">Prev</a>&nbsp;</td><th width="60%" align="center">Chapter&nbsp;2.&nbsp;Configuration</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="client_dependencies.html">Next</a></td></tr></table><hr></div><div class="section" title="2.4.&nbsp;The Important Configurations"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="important_configurations"></a>2.4.&nbsp;The Important Configurations</h2></div></div></div><p>Below we list the important Configurations.  We've divided this section into
      required configuration and worth-a-look recommended configs.
      </p><div class="section" title="2.4.1.&nbsp;Required Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="required_configuration"></a>2.4.1.&nbsp;Required Configurations</h3></div></div></div><p>See the <a class="link" href="notsoquick.html#requirements" title="1.3.1.&nbsp;Requirements">Requirements</a> section.
      It lists at least two required configurations needed running HBase bearing
      load: i.e. <a class="link" href="notsoquick.html#ulimit" title="1.3.1.6.&nbsp;ulimit">file descriptors <code class="varname">ulimit</code></a> and
      <a class="link" href="notsoquick.html#dfs.datanode.max.xcievers" title="1.3.1.7.&nbsp;dfs.datanode.max.xcievers"><code class="varname">dfs.datanode.max.xcievers</code></a>.
      </p></div><div class="section" title="2.4.2.&nbsp;Recommended Configuations"><div class="titlepage"><div><div><h3 class="title"><a name="recommended_configurations"></a>2.4.2.&nbsp;Recommended Configuations</h3></div></div></div><div class="section" title="2.4.2.1.&nbsp;zookeeper.session.timeout"><div class="titlepage"><div><div><h4 class="title"><a name="zookeeper.session.timeout"></a>2.4.2.1.&nbsp;<code class="varname">zookeeper.session.timeout</code></h4></div></div></div><p>The default timeout is three minutes (specified in milliseconds). This means
              that if a server crashes, it will be three minutes before the Master notices
              the crash and starts recovery. You might like to tune the timeout down to
              a minute or even less so the Master notices failures the sooner.
              Before changing this value, be sure you have your JVM garbage collection
              configuration under control otherwise, a long garbage collection that lasts
              beyond the zookeeper session timeout will take out
              your RegionServer (You might be fine with this -- you probably want recovery to start
          on the server if a RegionServer has been in GC for a long period of time).</p><p>To change this configuration, edit <code class="filename">hbase-site.xml</code>,
          copy the changed file around the cluster and restart.</p><p>We set this value high to save our having to field noob questions up on the mailing lists asking
              why a RegionServer went down during a massive import.  The usual cause is that their JVM is untuned and
              they are running into long GC pauses.  Our thinking is that
              while users are  getting familiar with HBase, we'd save them having to know all of its
              intricacies.  Later when they've built some confidence, then they can play
              with configuration such as this.
          </p></div><div class="section" title="2.4.2.2.&nbsp;hbase.regionserver.handler.count"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.handler.count"></a>2.4.2.2.&nbsp;<code class="varname">hbase.regionserver.handler.count</code></h4></div></div></div><p>
          This setting defines the number of threads that are kept open to answer
          incoming requests to user tables. The default of 10 is rather low in order to
          prevent users from killing their region servers when using large write buffers
          with a high number of concurrent clients. The rule of thumb is to keep this
          number low when the payload per request approaches the MB (big puts, scans using
          a large cache) and high when the payload is small (gets, small puts, ICVs, deletes).
          </p><p>
          It is safe to set that number to the
          maximum number of incoming clients if their payload is small, the typical example
          being a cluster that serves a website since puts aren't typically buffered
          and most of the operations are gets.
          </p><p>
          The reason why it is dangerous to keep this setting high is that the aggregate
          size of all the puts that are currently happening in a region server may impose
          too much pressure on its memory, or even trigger an OutOfMemoryError. A region server
          running on low memory will trigger its JVM's garbage collector to run more frequently
          up to a point where GC pauses become noticeable (the reason being that all the memory
          used to keep all the requests' payloads cannot be trashed, no matter how hard the
          garbage collector tries). After some time, the overall cluster
          throughput is affected since every request that hits that region server will take longer,
          which exacerbates the problem even more.
          </p></div><div class="section" title="2.4.2.3.&nbsp;Configuration for large memory machines"><div class="titlepage"><div><div><h4 class="title"><a name="big_memory"></a>2.4.2.3.&nbsp;Configuration for large memory machines</h4></div></div></div><p>
          HBase ships with a reasonable, conservative configuration that will
          work on nearly all
          machine types that people might want to test with. If you have larger
          machines you might the following configuration options helpful.
        </p></div><div class="section" title="2.4.2.4.&nbsp;LZO compression"><div class="titlepage"><div><div><h4 class="title"><a name="lzo"></a>2.4.2.4.&nbsp;LZO compression</h4></div></div></div><p>You should consider enabling LZO compression.  Its
      near-frictionless and in most all cases boosts performance.
      </p><p>Unfortunately, HBase cannot ship with LZO because of
      the licensing issues; HBase is Apache-licensed, LZO is GPL.
      Therefore LZO install is to be done post-HBase install.
      See the <a class="link" href="http://wiki.apache.org/hadoop/UsingLzoCompression" target="_top">Using LZO Compression</a>
      wiki page for how to make LZO work with HBase.
      </p><p>A common problem users run into when using LZO is that while initial
      setup of the cluster runs smooth, a month goes by and some sysadmin goes to
      add a machine to the cluster only they'll have forgotten to do the LZO
      fixup on the new machine.  In versions since HBase 0.90.0, we should
      fail in a way that makes it plain what the problem is, but maybe not.
      Remember you read this paragraph<sup>[<a name="d0e1798" href="#ftn.d0e1798" class="footnote">7</a>]</sup>.
      </p></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a name="ftn.d0e1798" href="#d0e1798" class="para">7</a>] </sup>See
      <a class="link" href="hbase.regionserver.codecs.html" title="B.2.&nbsp; hbase.regionserver.codecs">hbase.regionserver.codecs</a>
      for a feature to help protect against failed LZO install</p></div></div></div><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="log4j.html">Prev</a>&nbsp;</td><td width="20%" align="center"><a accesskey="u" href="configuration.html">Up</a></td><td width="40%" align="right">&nbsp;<a accesskey="n" href="client_dependencies.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">2.3.&nbsp;<code class="filename">log4j.properties</code>&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="book.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;2.5.&nbsp;Client configuration and dependencies connecting to an HBase cluster</td></tr></table></div></body></html>